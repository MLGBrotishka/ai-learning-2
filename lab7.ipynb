{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4ffb0d50-9e6d-4594-bff4-9ead1ad17da1",
      "metadata": {
        "id": "4ffb0d50-9e6d-4594-bff4-9ead1ad17da1"
      },
      "source": [
        "# Лабораторная работа №7 (Проведение исследований моделями семантической сегментации на наборе данных Oxford-IIIT Pet)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34971d28",
      "metadata": {},
      "source": [
        "\n",
        "## Выбор начальных условий\n",
        "\n",
        "### Выбор набора данных\n",
        "\n",
        "Выбранный набор данных: **Oxford-IIIT Pet Dataset**\n",
        "\n",
        "Обоснование выбора:\n",
        "- **Задача семантической сегментации:** Датасет Oxford-IIIT Pet, помимо меток классов изображений, также предоставляет пиксельные аннотации (трикарты - trimaps), которые четко разделяют пиксели, относящиеся к животному (pet), к фону (background), и к области, которую можно игнорировать (border/ignore). Это делает его идеальным выбором для задачи бинарной семантической сегментации (животное vs фон).\n",
        "- **Наличие аннотаций для сегментации:** Набор данных включает необходимые маски (trimaps) для обучения и оценки моделей сегментации на пиксельном уровне.\n",
        "- **Умеренная сложность:** Задача сегментации на этом датасете включает вариации поз животных, освещения и текстуры фона, представляя собой реалистичную, но не чрезмерно сложную задачу для демонстрации и сравнения моделей сегментации.\n",
        "- **Структурированные разделения:** Датасет предоставляет официальные разделения на тренировочный (trainval) и тестовый (test) наборы, что позволяет проводить стандартную процедуру обучения и валидации.\n",
        "- **Доступность:** Датасет легко доступен для скачивания и интеграции с помощью библиотек, таких как `torchvision`, как показано в коде.\n",
        "- **Бинарная задача с игнорированием:** Преобразование исходных меток маски (1, 2, 3) в целевые (1 для животного, 0 для фона, 255 для игнорируемой области) формирует четкую бинарную задачу сегментации с учетом \"неизвестных\" пикселей, что является распространенным сценарием в задачах сегментации.\n",
        "\n",
        "### Выбор метрик качества\n",
        "\n",
        "Выбранные метрики:\n",
        "- **Hamming Loss (Pixel Error):**\n",
        "  - Обоснование: Является обратной величиной Pixel Accuracy (1 - Pixel Accuracy). Показывает долю неправильно классифицированных пикселей. Дает простую и прямую оценку ошибки на пиксельном уровне. Важно учитывать, что эта метрика может быть завышена на классах с большим количеством пикселей (например, фон). Игнорируемые пиксели (метка 255) исключаются из расчета.\n",
        "- **F1 Score (Weighted Average):**\n",
        "  - Обоснование: Рассчитывается F1-оценка для каждого класса (животное, фон, исключая игнорируемый индекс 255), а затем эти оценки усредняются с весами, пропорциональными количеству истинных пикселей для каждого класса в тестовом наборе. Эта метрика учитывает как Precision (точность), так и Recall (полноту) для каждого класса и дает сбалансированную оценку производительности, смягчая влияние несбалансированности классов по площади (если один класс занимает значительно больше пикселей, чем другой).\n",
        "- **mAP (mean IoU - Mean Intersection over Union) (Macro Average):**\n",
        "  - Обоснование: Является стандартной метрикой для семантической сегментации. Рассчитывает IoU (отношение площади пересечения к площади объединения) для каждого класса (животное, фон, игнорируя индекс 255), а затем вычисляет среднее значение по всем классам. Это усреднение по классам (macro average) гарантирует, что каждый класс вносит равный вклад в итоговую метрику, независимо от его размера на изображениях. IoU является более строгой метрикой, чем Pixel Accuracy, так как она чувствительна к форме и точности границ сегментации.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e38803a1-29ef-4cfc-b9b5-15ad7d0b16a9",
      "metadata": {
        "id": "e38803a1-29ef-4cfc-b9b5-15ad7d0b16a9"
      },
      "source": [
        "## Подготовка и импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b794d1d-89d5-4e13-a6df-ea45625e0538",
      "metadata": {
        "id": "9b794d1d-89d5-4e13-a6df-ea45625e0538",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%pip install pillow\n",
        "%pip install pandas tabulate\n",
        "%pip install torchmetrics\n",
        "%pip install segmentation-models-pytorch\n",
        "%pip install einops\n",
        "%pip install opencv-python matplotlib scikit-image\n",
        "%pip install albumentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1730be0d-6d9b-404a-ade8-1ef8fc9e0360",
      "metadata": {
        "id": "1730be0d-6d9b-404a-ade8-1ef8fc9e0360",
        "tags": []
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import segmentation_models_pytorch as smp\n",
        "from torchvision import datasets\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tabulate import tabulate\n",
        "\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f3a68c-5317-4ce5-823c-0d701abf1210",
      "metadata": {
        "id": "30f3a68c-5317-4ce5-823c-0d701abf1210"
      },
      "source": [
        "## Вспомогательные функции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "2e5909f0-391e-40c5-9c3c-3b366cc6e26c",
      "metadata": {
        "id": "2e5909f0-391e-40c5-9c3c-3b366cc6e26c",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Обучает модель в течение одной эпохи.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, masks in train_loader:\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Убедимся, что тип данных маски - long для CrossEntropyLoss\n",
        "        masks = masks.long()\n",
        "\n",
        "        loss = criterion(outputs, masks)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Суммируем потери по батчу, взвешенные по размеру батча\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    print(f\"  Train Loss: {epoch_loss:.4f}\")\n",
        "    return epoch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a58e4f0-47b3-4b31-b1a4-8a452dd4bde3",
      "metadata": {
        "id": "7a58e4f0-47b3-4b31-b1a4-8a452dd4bde3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "def evaluate(model, val_loader, num_classes, device, ignore_index=255):\n",
        "    \"\"\"\n",
        "    Оценивает модель на валидационном наборе данных с использованием torchmetrics.\n",
        "    Рассчитывает Pixel Accuracy, F1 Score (weighted), и mAP (mean IoU).\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Инициализируем метрики torchmetrics, сохраняющие состояние\n",
        "    # JaccardIndex с average='macro' соответствует mean IoU\n",
        "    iou_metric = torchmetrics.JaccardIndex(\n",
        "        task='multiclass',\n",
        "        num_classes=num_classes,\n",
        "        ignore_index=ignore_index,\n",
        "        average='macro' # Используем 'macro' для mean IoU\n",
        "    ).to(device) # Метрики также могут находиться на устройстве GPU\n",
        "\n",
        "    # F1Score с average='weighted' учитывает дисбаланс классов на основе количества примеров\n",
        "    f1_metric = torchmetrics.F1Score(\n",
        "        task='multiclass',\n",
        "        num_classes=num_classes,\n",
        "        ignore_index=ignore_index,\n",
        "        average='weighted'\n",
        "    ).to(device)\n",
        "\n",
        "    # Accuracy для расчета Pixel Accuracy\n",
        "    pixel_accuracy_metric = torchmetrics.Accuracy(\n",
        "        task='multiclass',\n",
        "        num_classes=num_classes,\n",
        "        ignore_index=ignore_index\n",
        "    ).to(device)\n",
        "\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in val_loader:\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Получаем предсказания (индекс класса с наивысшим скором)\n",
        "            preds = torch.argmax(outputs, dim=1) # Форма: (N, H, W)\n",
        "\n",
        "            # Убедимся, что маски имеют тип long для метрик (должны быть уже, но это хорошая практика)\n",
        "            masks = masks.long()\n",
        "\n",
        "            # Обновляем метрики текущим батчем предсказаний и ground truth\n",
        "            iou_metric.update(preds, masks)\n",
        "            f1_metric.update(preds, masks)\n",
        "            pixel_accuracy_metric.update(preds, masks)\n",
        "\n",
        "    # Расчет метрик\n",
        "    # Вычисляем финальные значения метрик из накопленного состояния\n",
        "    mAP = iou_metric.compute().item()      # mean IoU\n",
        "    f1 = f1_metric.compute().item()        # Weighted F1 Score\n",
        "    pixel_accuracy = pixel_accuracy_metric.compute().item() # Pixel Accuracy\n",
        "\n",
        "    # Hamming Loss равен 1 - Pixel Accuracy\n",
        "    h_loss = 1.0 - pixel_accuracy\n",
        "\n",
        "    # Сбрасываем состояние метрик для следующего запуска оценки (важно, если вызывается evaluate несколько раз)\n",
        "    iou_metric.reset()\n",
        "    f1_metric.reset()\n",
        "    pixel_accuracy_metric.reset()\n",
        "\n",
        "    print(f\"  Hamming Loss (Pixel Error): {h_loss:.4f}\")\n",
        "    print(f\"  F1 Score (Weighted): {f1:.4f}\")\n",
        "    print(f\"  mAP (mean IoU): {mAP:.4f}\")\n",
        "\n",
        "    return h_loss, f1, mAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "train_model_function",
      "metadata": {
        "id": "train_model_function"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_model(model, train_loader, val_loader, criterion, optimizer, device, epochs, num_classes, model_name, ignore_index=255):\n",
        "    \"\"\"\n",
        "    Обучает модель заданное количество эпох и оценивает ее после обучения.\n",
        "    Возвращает финальные метрики валидации.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Training {model_name} ---\")\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"EPOCH {epoch+1}/{epochs}\")\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "    print(f\"Training {model_name} finished.\")\n",
        "    print(\"Evaluating on test set...\")\n",
        "    h_loss, f1, mAP = evaluate(model, val_loader, num_classes, device, ignore_index=ignore_index)\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    return h_loss, f1, mAP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cddffae6-193d-45dc-b6f8-0f529d223f27",
      "metadata": {
        "id": "cddffae6-193d-45dc-b6f8-0f529d223f27"
      },
      "source": [
        "## Настройка параметров и датасета Oxford-IIIT Pet (Semantic Segmentation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "604ec335-e495-4cb0-bf02-81identité2361a4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "604ec335-e495-4cb0-bf02-81identité2361a4d",
        "outputId": "215dfd68-64bb-4e63-9c18-eeb1e9f5dcbd",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Параметры\n",
        "# Классы: 0 - фон, 1 - пиксели животного\n",
        "CLASSES = [\"background\", \"pet\"]\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "INPUT_DIM = (224, 224) # Меньший размер для ускорения обучения\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 0.0001 # Базовый LR для предобученных моделей\n",
        "BASE_EPOCHS = 5 # Базовое количество эпох для бейзлайна и первых тестов\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Маппинг значений маски: Original (1=pet, 2=background, 3=ignore) -> Target (1=pet, 0=background, 255=ignore)\n",
        "MASK_MAPPING = { 1: 1, 2: 0, 3: 255 }\n",
        "IGNORE_INDEX = 255"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "oxford_pet_dataset_class",
      "metadata": {
        "id": "oxford_pet_dataset_class"
      },
      "outputs": [],
      "source": [
        "class OxfordPetSegmentationDataset(Dataset):\n",
        "    def __init__(self, data_dir, image_set=\"trainval\", transform=None, mask_mapping=None, ignore_index=255):\n",
        "        self.data_dir = data_dir\n",
        "        self.image_set = image_set\n",
        "        self.transform = transform\n",
        "        self.mask_mapping = mask_mapping if mask_mapping is not None else {}\n",
        "        self.ignore_index = ignore_index\n",
        "\n",
        "        self.image_dir = os.path.join(data_dir, 'images')\n",
        "        self.mask_dir = os.path.join(data_dir, 'annotations', 'trimaps')\n",
        "\n",
        "        # Загружаем имена файлов изображений из официальных сплитов\n",
        "        list_file = os.path.join(data_dir, 'annotations', f'{image_set}.txt')\n",
        "\n",
        "        if not os.path.exists(list_file):\n",
        "             raise FileNotFoundError(f\"Файл сплита {list_file} не найден. Пожалуйста, скачайте датасет.\")\n",
        "\n",
        "        image_basenames = []\n",
        "        with open(list_file, 'r') as f:\n",
        "            for line in f:\n",
        "                 line = line.strip()\n",
        "                 if line:\n",
        "                     image_basenames.append(line.split()[0])\n",
        "\n",
        "        self.items = []\n",
        "        for basename in image_basenames:\n",
        "            img_path = os.path.join(self.image_dir, basename + '.jpg')\n",
        "            mask_path = os.path.join(self.mask_dir, basename + '.png')\n",
        "\n",
        "            if os.path.exists(img_path) and os.path.exists(mask_path):\n",
        "                 self.items.append((img_path, mask_path))\n",
        "            else:\n",
        "                 print(f\"Отсутствует файл(ы) для {basename}: {img_path} или {mask_path}\")\n",
        "\n",
        "        print(f\"Загружено {len(self.items)} объектов для сплита {image_set}.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.items)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, mask_path = self.items[idx]\n",
        "\n",
        "        # Загружаем изображение (Убедимся в формате RGB)\n",
        "        img = np.array(Image.open(img_path).convert('RGB'))\n",
        "\n",
        "        # Загружаем маску (оттенки серого). Тримапы имеют значения 1, 2, 3.\n",
        "        mask = np.array(Image.open(mask_path).convert('L'), dtype=np.uint8)\n",
        "\n",
        "        # Применяем маппинг маски: Original (1=pet, 2=background, 3=ignore) -> Target (1=pet, 0=background, 255=ignore)\n",
        "        processed_mask = np.full_like(mask, self.ignore_index, dtype=np.uint8)\n",
        "\n",
        "        # Применяем маппинг для известных значений\n",
        "        for original_value, target_value in self.mask_mapping.items():\n",
        "             processed_mask[mask == original_value] = target_value\n",
        "\n",
        "        if 3 in self.mask_mapping:\n",
        "             processed_mask[mask == 3] = self.mask_mapping[3]\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=img, mask=processed_mask)\n",
        "            img = augmented['image'] # Должен быть тензором (C, H, W) из-за ToTensorV2\n",
        "            mask = augmented['mask'] # Должен быть тензором (H, W) из-за ToTensorV2 (transpose_mask=False по умолчанию)\n",
        "\n",
        "        # Albumentations ToTensorV2 по умолчанию возвращает маску как float32. Преобразуем в long.\n",
        "        mask = mask.long()\n",
        "\n",
        "        return img, mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "augmentations_definition",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "augmentations_definition",
        "outputId": "a5fc41c8-b5c0-4e5e-e439-7cdefc852dc7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:111: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
            "  original_init(self, **validated_kwargs)\n"
          ]
        }
      ],
      "source": [
        "# Определение пайплайнов аугментации и преобразования\n",
        "\n",
        "# Базовые преобразования (для валидации и не-аугментированного обучения)\n",
        "base_transform = A.Compose([\n",
        "    A.Resize(*INPUT_DIM),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(), # Преобразует изображение в тензор C,H,W float; маску в тензор H,W float\n",
        "])\n",
        "\n",
        "# Аугментации для тренировочного набора\n",
        "augmentations = A.Compose([\n",
        "    A.Resize(*INPUT_DIM), # Применяем Resize первым, чтобы другие аугментации работали с правильными размерами\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.1),\n",
        "    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=5, p=0.5),\n",
        "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ToTensorV2(), # Преобразует изображение в тензор C,H,W float; маску в тензор H,W float\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dataset_loaders_initial",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dataset_loaders_initial",
        "outputId": "80a9c4f3-9b55-4cd9-a3a7-7d36247b87ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets (base transforms)...\n",
            "Loaded 3680 items for trainval set.\n",
            "Loaded 3669 items for test set.\n",
            "Train loader size: 3680\n",
            "Test loader size: 3669\n"
          ]
        }
      ],
      "source": [
        "# Загрузка датасетов и создание DataLoader'ов (для базового обучения)\n",
        "datasets.OxfordIIITPet(root='./data', split='trainval', download=True)\n",
        "\n",
        "print(\"Loading datasets (base transforms)...\")\n",
        "train_dataset_base = OxfordPetSegmentationDataset(\n",
        "    data_dir=\"./data/oxford-iiit-pet\",\n",
        "    image_set=\"trainval\", # Oxford имеет сплиты trainval и test\n",
        "    transform=base_transform,\n",
        "    mask_mapping=MASK_MAPPING,\n",
        "    ignore_index=IGNORE_INDEX\n",
        ")\n",
        "test_dataset = OxfordPetSegmentationDataset(\n",
        "    data_dir=\"./data/oxford-iiit-pet\",\n",
        "    image_set=\"test\",\n",
        "    transform=base_transform,\n",
        "    mask_mapping=MASK_MAPPING,\n",
        "    ignore_index=IGNORE_INDEX\n",
        ")\n",
        "\n",
        "train_loader_base = DataLoader(\n",
        "    dataset=train_dataset_base,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2 # Рассмотрите увеличение num_workers, если CPU является узким местом\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"Train loader size: {len(train_loader_base.dataset)}\")\n",
        "print(f\"Test loader size: {len(test_loader.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbe9487d-1f71-4b5a-af73-d2d7d463cc8a",
      "metadata": {
        "id": "bbe9487d-1f71-4b5a-af73-d2d7d463cc8a"
      },
      "source": [
        "## Бейзлайн эксперименты (готовые модели из библиотеки)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce2e9ef2-8c0d-4ac3-89d4-4e6fc87d6bf2",
      "metadata": {
        "id": "ce2e9ef2-8c0d-4ac3-89d4-4e6fc87d6bf2"
      },
      "source": [
        "Инициализация моделей Unet и Segformer из `segmentation-models-pytorch`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5caafae1-a27a-4744-b6de-16c7a377e7bf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5caafae1-a27a-4744-b6de-16c7a377e7bf",
        "outputId": "478677d2-0da9-45d2-f9f0-f90e7e1720d4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Инициализация Unet\n",
        "model_unet_base = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\", # Используем предобученные веса\n",
        "    in_channels=3,\n",
        "    classes=NUM_CLASSES,\n",
        "    activation=None # None для CrossEntropyLoss (функция активации применяется внутри лосса)\n",
        ")\n",
        "model_unet_base.to(DEVICE);\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=IGNORE_INDEX)\n",
        "optimizer_unet_base = optim.Adam(model_unet_base.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "segformer_base_init",
      "metadata": {
        "id": "segformer_base_init"
      },
      "outputs": [],
      "source": [
        "# Инициализация Segformer\n",
        "# Segformer обычно использует энкодеры MixTransformer (mit_b0 до mit_b5).\n",
        "# Resnet34 - стандартный CNN энкодер. Давайте используем типичный энкодер Segformer.\n",
        "# Используем mit_b0, так как он относительно небольшой.\n",
        "model_segformer_base = smp.Segformer(\n",
        "    encoder_name=\"mit_b0\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=NUM_CLASSES,\n",
        "    # Segformer в smp по умолчанию использует Softmax в конце декодера,\n",
        "    # поэтому activation=None не требуется, если используется CrossEntropyLoss.\n",
        "    # Если бы использовались другие лоссы, мог бы потребоваться 'sigmoid'.\n",
        ")\n",
        "model_segformer_base.to(DEVICE);\n",
        "\n",
        "optimizer_segformer_base = optim.Adam(model_segformer_base.parameters(), lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b033246b-0e9e-4b5e-9394-bfbbbe05bf14",
      "metadata": {
        "id": "b033246b-0e9e-4b5e-9394-bfbbbe05bf14"
      },
      "source": [
        "Обучение и оценка базовой модели Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0YGMXJ5M3b02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YGMXJ5M3b02",
        "outputId": "2d3c618d-4577-4036-d9e8-c19c3b36be35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Evaluating Untrained Unet Baseline ---\n",
            "  Hamming Loss (Pixel Error): 0.4115\n",
            "  F1 Score (Weighted): 0.5718\n",
            "  mAP (mean IoU): 0.3656\n",
            "\n",
            "--- Evaluating Untrained Segformer Baseline ---\n",
            "  Hamming Loss (Pixel Error): 0.5706\n",
            "  F1 Score (Weighted): 0.4311\n",
            "  mAP (mean IoU): 0.2733\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Evaluating Untrained Unet Baseline\")\n",
        "h_loss_unet_untrained, f1_unet_untrained, mAP_unet_untrained = evaluate(\n",
        "    model=model_unet_base, # Используем модель сразу после инициализации\n",
        "    val_loader=test_loader,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    device=DEVICE,\n",
        "    ignore_index=IGNORE_INDEX\n",
        ")\n",
        "\n",
        "print(\"\\n--- Evaluating Untrained Segformer Baseline ---\")\n",
        "h_loss_segformer_untrained, f1_segformer_untrained, mAP_segformer_untrained = evaluate(\n",
        "    model=model_segformer_base, # Используем модель сразу после инициализации\n",
        "    val_loader=test_loader,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    device=DEVICE,\n",
        "    ignore_index=IGNORE_INDEX\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4386cdc9-1f9b-401c-92ca-4fd40fb85e77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4386cdc9-1f9b-401c-92ca-4fd40fb85e77",
        "outputId": "27cb95b7-9bdf-4167-c87a-d8b360b3c860",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Unet (base) ---\n",
            "EPOCH 1/5\n",
            "  Train Loss: 0.1729\n",
            "EPOCH 2/5\n",
            "  Train Loss: 0.0564\n",
            "EPOCH 3/5\n",
            "  Train Loss: 0.0371\n",
            "EPOCH 4/5\n",
            "  Train Loss: 0.0277\n",
            "EPOCH 5/5\n",
            "  Train Loss: 0.0243\n",
            "Training Unet (base) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.0145\n",
            "  F1 Score (Weighted): 0.9855\n",
            "  mAP (mean IoU): 0.9684\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "h_loss_unet_base, f1_unet_base, mAP_unet_base = train_and_evaluate_model(\n",
        "    model=model_unet_base,\n",
        "    train_loader=train_loader_base,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_unet_base,\n",
        "    device=DEVICE,\n",
        "    epochs=BASE_EPOCHS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Unet (base)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edefb2a0-5846-4673-8c79-21dc01706c8f",
      "metadata": {
        "id": "edefb2a0-5846-4673-8c79-21dc01706c8f"
      },
      "source": [
        "Обучение и оценка базовой модели Segformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "2733c9d8-1366-4320-9fcd-4327bee5c0fb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2733c9d8-1366-4320-9fcd-4327bee5c0fb",
        "outputId": "8c73cc39-b5b9-456e-dd51-75437a40ad83",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Segformer (base) ---\n",
            "EPOCH 1/5\n",
            "  Train Loss: 0.1076\n",
            "EPOCH 2/5\n",
            "  Train Loss: 0.0512\n",
            "EPOCH 3/5\n",
            "  Train Loss: 0.0403\n",
            "EPOCH 4/5\n",
            "  Train Loss: 0.0281\n",
            "EPOCH 5/5\n",
            "  Train Loss: 0.0241\n",
            "Training Segformer (base) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.0133\n",
            "  F1 Score (Weighted): 0.9868\n",
            "  mAP (mean IoU): 0.9711\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "h_loss_segformer_base, f1_segformer_base, mAP_segformer_base = train_and_evaluate_model(\n",
        "    model=model_segformer_base,\n",
        "    train_loader=train_loader_base,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_segformer_base,\n",
        "    device=DEVICE,\n",
        "    epochs=BASE_EPOCHS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Segformer (base)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a8ec66-4b7a-4e93-8615-a98438a98446",
      "metadata": {
        "id": "74a8ec66-4b7a-4e93-8615-a98438a98446"
      },
      "source": [
        "## Улучшение бейзлайна"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dca641ff-fe0f-403e-9668-a1847d99e93e",
      "metadata": {
        "id": "dca641ff-fe0f-403e-9668-a1847d99e93e"
      },
      "source": [
        "### Формулирование гипотез"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hypotheses",
      "metadata": {
        "id": "hypotheses"
      },
      "source": [
        "На основе предыдущей лабораторной работы по классификации, где аугментация данных и подбор гиперпараметров значительно улучшили качество моделей, сформулируем аналогичные гипотезы для семантической сегментации.\n",
        "\n",
        "#### Гипотеза 1: Аугментация данных\n",
        "\n",
        "Цель: Повышение генерализирующей способности моделей семантической сегментации. Поскольку сегментация требует сохранения пространственной согласованности между изображением и маской, аугментации должны применяться синхронно к обоим.\n",
        "\n",
        "Предполагаемый эффект: Уменьшение переобучения на тренировочном наборе, повышение устойчивости к вариациям в тестовых данных (поворот, масштабирование, изменение освещенности).\n",
        "\n",
        "#### Гипотеза 2: Подбор гиперпараметров и оптимизатора\n",
        "\n",
        "Цель: Найти более эффективные параметры обучения для дообучения предобученных моделей.\n",
        "\n",
        "Идеи для реализации:\n",
        "*   Увеличение количества эпох обучения: Для более тщательной настройки весов на целевом датасете.\n",
        "*   Снижение Learning Rate: Меньший шаг оптимизации для тонкой настройки предобученных весов.\n",
        "*   Смена оптимизатора: Возможно, AdamW, часто более эффективный для моделей на основе трансформеров (как Segformer), также подойдет и для сверточных моделей.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21c0a7f0-4ca4-4219-9ad3-c791fdbd27db",
      "metadata": {
        "id": "21c0a7f0-4ca4-4219-9ad3-c791fdbd27db"
      },
      "source": [
        "### Проверка гипотез"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c31fbd0a-01e8-481c-92a0-328854a8bc1a",
      "metadata": {
        "id": "c31fbd0a-01e8-481c-92a0-328854a8bc1a"
      },
      "source": [
        "#### Гипотеза 1: Аугментация данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "augmented_dataset_loader",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "augmented_dataset_loader",
        "outputId": "92f2cae1-ddcb-40e9-9843-96ffb7cd4b6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training dataset with augmentations...\n",
            "Loaded 3680 items for trainval set.\n",
            "Augmented train loader size: 3680\n"
          ]
        }
      ],
      "source": [
        "# Создание датасета с аугментацией для тренировочного набора\n",
        "print(\"Loading training dataset with augmentations...\")\n",
        "train_dataset_aug = OxfordPetSegmentationDataset(\n",
        "    data_dir=\"./data/oxford-iiit-pet\",\n",
        "    image_set=\"trainval\",\n",
        "    transform=augmentations,\n",
        "    mask_mapping=MASK_MAPPING,\n",
        "    ignore_index=IGNORE_INDEX\n",
        ")\n",
        "\n",
        "train_loader_aug = DataLoader(\n",
        "    dataset=train_dataset_aug,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "print(f\"Augmented train loader size: {len(train_loader_aug.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "214ed32f-7b2d-49c2-8ced-eb288aba9afc",
      "metadata": {
        "id": "214ed32f-7b2d-49c2-8ced-eb288aba9afc"
      },
      "source": [
        "Обучение и оценка Unet с аугментацией (используем базовые HPs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "unet_aug_test",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unet_aug_test",
        "outputId": "86cd7c88-0b1a-4ec6-efbf-e8019f313604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Unet (aug) ---\n",
            "EPOCH 1/5\n",
            "  Train Loss: 0.1795\n",
            "EPOCH 2/5\n",
            "  Train Loss: 0.0633\n",
            "EPOCH 3/5\n",
            "  Train Loss: 0.0488\n",
            "EPOCH 4/5\n",
            "  Train Loss: 0.0410\n",
            "EPOCH 5/5\n",
            "  Train Loss: 0.0305\n",
            "Training Unet (aug) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.0157\n",
            "  F1 Score (Weighted): 0.9843\n",
            "  mAP (mean IoU): 0.9658\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Инициализация Unet (начинаем с чистого листа)\n",
        "model_unet_aug = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=NUM_CLASSES,\n",
        "    activation=None\n",
        ")\n",
        "model_unet_aug.to(DEVICE);\n",
        "# Используем базовый LR и Adam\n",
        "optimizer_unet_aug = optim.Adam(model_unet_aug.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "h_loss_unet_aug, f1_unet_aug, mAP_unet_aug = train_and_evaluate_model(\n",
        "    model=model_unet_aug,\n",
        "    train_loader=train_loader_aug,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_unet_aug,\n",
        "    device=DEVICE,\n",
        "    epochs=BASE_EPOCHS, # Столько же эпох, как в бейзлайне\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Unet (aug)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f6f5003-4528-4d4f-b158-cbaa5238b3dc",
      "metadata": {
        "id": "4f6f5003-4528-4d4f-b158-cbaa5238b3dc"
      },
      "source": [
        "Обучение и оценка Segformer с аугментацией (используем базовые HPs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "segformer_aug_test",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "segformer_aug_test",
        "outputId": "b94ea63b-48c7-448d-df43-d5aebfbf1579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Segformer (aug) ---\n",
            "EPOCH 1/5\n",
            "  Train Loss: 0.1019\n",
            "EPOCH 2/5\n",
            "  Train Loss: 0.0582\n",
            "EPOCH 3/5\n",
            "  Train Loss: 0.0465\n",
            "EPOCH 4/5\n",
            "  Train Loss: 0.0402\n",
            "EPOCH 5/5\n",
            "  Train Loss: 0.0331\n",
            "Training Segformer (aug) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.0133\n",
            "  F1 Score (Weighted): 0.9867\n",
            "  mAP (mean IoU): 0.9710\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Инициализация Segformer (начинаем с чистого листа)\n",
        "model_segformer_aug = smp.Segformer(\n",
        "    encoder_name=\"mit_b0\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=NUM_CLASSES,\n",
        ")\n",
        "model_segformer_aug.to(DEVICE);\n",
        "# Используем базовый LR и Adam\n",
        "optimizer_segformer_aug = optim.Adam(model_segformer_aug.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "h_loss_segformer_aug, f1_segformer_aug, mAP_segformer_aug = train_and_evaluate_model(\n",
        "    model=model_segformer_aug,\n",
        "    train_loader=train_loader_aug,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_segformer_aug,\n",
        "    device=DEVICE,\n",
        "    epochs=BASE_EPOCHS, # Столько же эпох, как в бейзлайне\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Segformer (aug)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21eb0d81-5e5e-4029-ab9b-e2198cad91aa",
      "metadata": {
        "id": "21eb0d81-5e5e-4029-ab9b-e2198cad91aa"
      },
      "source": [
        "#### Гипотеза 2: Подбор гиперпараметров и оптимизатора"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "hyperparameter_tuning_epochs",
      "metadata": {
        "id": "hyperparameter_tuning_epochs"
      },
      "outputs": [],
      "source": [
        "# Увеличиваем количество эпох и уменьшаем LR для проверки гипотезы\n",
        "HP_TUNING_EPOCHS = 10 # Увеличиваем количество эпох\n",
        "HP_TUNING_LEARNING_RATE = 0.00005 # Уменьшаем LR\n",
        "\n",
        "# Используем базовый датасет без аугментации для изоляции эффекта HPs\n",
        "train_loader_hp = train_loader_base"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2718a902-ba72-49d5-ba57-176cf7cbb8ea",
      "metadata": {
        "id": "2718a902-ba72-49d5-ba57-176cf7cbb8ea"
      },
      "source": [
        "Обучение и оценка Unet с новыми HPs (без аугментации)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "unet_hp_epochs_test",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unet_hp_epochs_test",
        "outputId": "7ce59a52-2603-46bd-97de-b9c94fbfa536"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Unet (HP) ---\n",
            "EPOCH 1/10\n",
            "  Train Loss: 0.2108\n",
            "EPOCH 2/10\n",
            "  Train Loss: 0.0661\n",
            "EPOCH 3/10\n",
            "  Train Loss: 0.0387\n",
            "EPOCH 4/10\n",
            "  Train Loss: 0.0252\n",
            "EPOCH 5/10\n",
            "  Train Loss: 0.0189\n",
            "EPOCH 6/10\n",
            "  Train Loss: 0.0152\n",
            "EPOCH 7/10\n",
            "  Train Loss: 0.0114\n",
            "EPOCH 8/10\n",
            "  Train Loss: 0.0101\n",
            "EPOCH 9/10\n",
            "  Train Loss: 0.0178\n",
            "EPOCH 10/10\n",
            "  Train Loss: 0.0084\n",
            "Training Unet (HP) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.0127\n",
            "  F1 Score (Weighted): 0.9873\n",
            "  mAP (mean IoU): 0.9721\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Инициализация Unet (начинаем с чистого листа)\n",
        "model_unet_hp = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=NUM_CLASSES,\n",
        "    activation=None\n",
        ")\n",
        "model_unet_hp.to(DEVICE);\n",
        "optimizer_unet_hp = optim.Adam(model_unet_hp.parameters(), lr=HP_TUNING_LEARNING_RATE)\n",
        "\n",
        "h_loss_unet_hp, f1_unet_hp, mAP_unet_hp = train_and_evaluate_model(\n",
        "    model=model_unet_hp,\n",
        "    train_loader=train_loader_hp,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_unet_hp,\n",
        "    device=DEVICE,\n",
        "    epochs=HP_TUNING_EPOCHS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Unet (HP)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb6fa1da-350f-4063-ae8c-a4eb210edc02",
      "metadata": {
        "id": "fb6fa1da-350f-4063-ae8c-a4eb210edc02"
      },
      "source": [
        "Обучение и оценка Segformer с новыми HPs (без аугментации)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "segformer_hp_epochs_test",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "segformer_hp_epochs_test",
        "outputId": "40a6ccb3-e33b-4516-d09f-b38376b81990"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Segformer (HP) ---\n",
            "EPOCH 1/10\n",
            "  Train Loss: 0.1405\n",
            "EPOCH 2/10\n",
            "  Train Loss: 0.0599\n",
            "EPOCH 3/10\n",
            "  Train Loss: 0.0466\n",
            "EPOCH 4/10\n",
            "  Train Loss: 0.0366\n",
            "EPOCH 5/10\n",
            "  Train Loss: 0.0295\n",
            "EPOCH 6/10\n",
            "  Train Loss: 0.0269\n",
            "EPOCH 7/10\n",
            "  Train Loss: 0.0245\n",
            "EPOCH 8/10\n",
            "  Train Loss: 0.0213\n",
            "EPOCH 9/10\n",
            "  Train Loss: 0.0208\n",
            "EPOCH 10/10\n",
            "  Train Loss: 0.0180\n",
            "Training Segformer (HP) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.0125\n",
            "  F1 Score (Weighted): 0.9875\n",
            "  mAP (mean IoU): 0.9727\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Инициализация Segformer (начинаем с чистого листа)\n",
        "model_segformer_hp = smp.Segformer(\n",
        "    encoder_name=\"mit_b0\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=NUM_CLASSES,\n",
        ")\n",
        "model_segformer_hp.to(DEVICE);\n",
        "# Используем AdamW для трансформера с пониженным LR\n",
        "optimizer_segformer_hp = optim.AdamW(model_segformer_hp.parameters(), lr=HP_TUNING_LEARNING_RATE, weight_decay=0.01)\n",
        "\n",
        "h_loss_segformer_hp, f1_segformer_hp, mAP_segformer_hp = train_and_evaluate_model(\n",
        "    model=model_segformer_hp,\n",
        "    train_loader=train_loader_hp,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_segformer_hp,\n",
        "    device=DEVICE,\n",
        "    epochs=HP_TUNING_EPOCHS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Segformer (HP)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11956431-967a-4403-92cc-ab17c3436f31",
      "metadata": {
        "id": "11956431-967a-4403-92cc-ab17c3436f31"
      },
      "source": [
        "### Формирование улучшенного бейзлайна"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "improved_baseline_description",
      "metadata": {
        "id": "improved_baseline_description"
      },
      "source": [
        "На основе результатов проверки гипотез, мы объединим наиболее успешные техники для формирования \"улучшенного\" бейзлайна.\n",
        "\n",
        "Ожидается, что комбинация **аугментации данных** с **улучшенными гиперпараметрами (пониженный LR, увеличенные эпохи, AdamW для Segformer)** даст наилучший результат для предобученных моделей из `segmentation-models-pytorch`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "improved_baseline_params_data",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "improved_baseline_params_data",
        "outputId": "d5a32411-ea12-4026-bbc9-ed7809eb2e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Настройки улучшенного бейзлайна: Аугментация, LR=5e-05, Epochs=10\n"
          ]
        }
      ],
      "source": [
        "# Определяем окончательные параметры для улучшенного бейзлайна\n",
        "FINAL_EPOCHS = 10 # Увеличиваем количество эпох для финального прогона\n",
        "FINAL_LEARNING_RATE = HP_TUNING_LEARNING_RATE # Используем пониженный LR\n",
        "\n",
        "# Используем датасет с аугментацией\n",
        "train_loader_improved = train_loader_aug\n",
        "\n",
        "print(\"Настройки улучшенного бейзлайна: Аугментация, LR={FINAL_LEARNING_RATE}, Epochs={FINAL_EPOCHS}\".format(FINAL_LEARNING_RATE=FINAL_LEARNING_RATE, FINAL_EPOCHS=FINAL_EPOCHS))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e954a86-9893-4285-ab5e-597e5b41af91",
      "metadata": {
        "id": "1e954a86-9893-4285-ab5e-597e5b41af91"
      },
      "source": [
        "Обучение и оценка улучшенной модели Unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "unet_improved_train_eval",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unet_improved_train_eval",
        "outputId": "057effba-537d-45f7-abc6-170adf2e7508"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Unet (improved) ---\n",
            "EPOCH 1/10\n",
            "  Train Loss: 0.2295\n",
            "EPOCH 2/10\n",
            "  Train Loss: 0.0830\n",
            "EPOCH 3/10\n",
            "  Train Loss: 0.0561\n",
            "EPOCH 4/10\n",
            "  Train Loss: 0.0409\n",
            "EPOCH 5/10\n",
            "  Train Loss: 0.0351\n",
            "EPOCH 6/10\n",
            "  Train Loss: 0.0298\n",
            "EPOCH 7/10\n",
            "  Train Loss: 0.0247\n",
            "EPOCH 8/10\n",
            "  Train Loss: 0.0200\n",
            "EPOCH 9/10\n",
            "  Train Loss: 0.0184\n",
            "EPOCH 10/10\n",
            "  Train Loss: 0.0147\n",
            "Training Unet (improved) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.0113\n",
            "  F1 Score (Weighted): 0.9887\n",
            "  mAP (mean IoU): 0.9752\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Инициализация Unet (начинаем с чистого листа)\n",
        "model_unet_improved = smp.Unet(\n",
        "    encoder_name=\"resnet34\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=NUM_CLASSES,\n",
        "    activation=None\n",
        ")\n",
        "model_unet_improved.to(DEVICE);\n",
        "optimizer_unet_improved = optim.Adam(model_unet_improved.parameters(), lr=FINAL_LEARNING_RATE)\n",
        "\n",
        "h_loss_unet_improved, f1_unet_improved, mAP_unet_improved = train_and_evaluate_model(\n",
        "    model=model_unet_improved,\n",
        "    train_loader=train_loader_improved,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_unet_improved,\n",
        "    device=DEVICE,\n",
        "    epochs=FINAL_EPOCHS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Unet (improved)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9971134b-a139-4548-a953-0e84610c3510",
      "metadata": {
        "id": "9971134b-a139-4548-a953-0e84610c3510"
      },
      "source": [
        "Обучение и оценка улучшенной модели Segformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "segformer_improved_train_eval",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "segformer_improved_train_eval",
        "outputId": "f31727fa-7bb7-4436-81ca-bec663c6011c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Segformer (improved) ---\n",
            "EPOCH 1/10\n",
            "  Train Loss: 0.1253\n",
            "EPOCH 2/10\n",
            "  Train Loss: 0.0619\n",
            "EPOCH 3/10\n",
            "  Train Loss: 0.0527\n",
            "EPOCH 4/10\n",
            "  Train Loss: 0.0460\n",
            "EPOCH 5/10\n",
            "  Train Loss: 0.0384\n",
            "EPOCH 6/10\n",
            "  Train Loss: 0.0366\n",
            "EPOCH 7/10\n",
            "  Train Loss: 0.0331\n",
            "EPOCH 8/10\n",
            "  Train Loss: 0.0296\n",
            "EPOCH 9/10\n",
            "  Train Loss: 0.0289\n",
            "EPOCH 10/10\n",
            "  Train Loss: 0.0275\n",
            "Training Segformer (improved) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.0119\n",
            "  F1 Score (Weighted): 0.9881\n",
            "  mAP (mean IoU): 0.9738\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Инициализация Segformer (начинаем с чистого листа)\n",
        "model_segformer_improved = smp.Segformer(\n",
        "    encoder_name=\"mit_b0\",\n",
        "    encoder_weights=\"imagenet\",\n",
        "    in_channels=3,\n",
        "    classes=NUM_CLASSES,\n",
        ")\n",
        "model_segformer_improved.to(DEVICE);\n",
        "# Используем AdamW\n",
        "optimizer_segformer_improved = optim.AdamW(model_segformer_improved.parameters(), lr=FINAL_LEARNING_RATE, weight_decay=0.01)\n",
        "\n",
        "h_loss_segformer_improved, f1_segformer_improved, mAP_segformer_improved = train_and_evaluate_model(\n",
        "    model=model_segformer_improved,\n",
        "    train_loader=train_loader_improved,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_segformer_improved,\n",
        "    device=DEVICE,\n",
        "    epochs=FINAL_EPOCHS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Segformer (improved)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1fbd97b-ed58-4682-8632-6ba32ba3ac5e",
      "metadata": {
        "id": "f1fbd97b-ed58-4682-8632-6ba32ba3ac5e"
      },
      "source": [
        "### Сравнение результатов бейзлайн моделей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "baseline_comparison_table",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baseline_comparison_table",
        "outputId": "f13b8a43-f0a6-4759-a685-7aa4eb56a594"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Сравнение результатов предобученных моделей:\n",
            "|    | Model                |   Hamming Loss (Pixel Error) |   F1 Score (Weighted) |   mAP (mean IoU) |\n",
            "|---:|:---------------------|-----------------------------:|----------------------:|-----------------:|\n",
            "|  0 | Unet (base)          |                       0.0145 |                0.9855 |           0.9684 |\n",
            "|  1 | Unet (aug)           |                       0.0157 |                0.9843 |           0.9658 |\n",
            "|  2 | Unet (HP)            |                       0.0127 |                0.9873 |           0.9721 |\n",
            "|  3 | Unet (improved)      |                       0.0113 |                0.9887 |           0.9752 |\n",
            "|  4 | Segformer (base)     |                       0.0133 |                0.9868 |           0.9711 |\n",
            "|  5 | Segformer (aug)      |                       0.0133 |                0.9867 |           0.971  |\n",
            "|  6 | Segformer (HP)       |                       0.0125 |                0.9875 |           0.9727 |\n",
            "|  7 | Segformer (improved) |                       0.0119 |                0.9881 |           0.9738 |\n"
          ]
        }
      ],
      "source": [
        "results = {\n",
        "    'Model': ['Unet (base)', 'Unet (aug)', 'Unet (HP)', 'Unet (improved)',\n",
        "              'Segformer (base)', 'Segformer (aug)', 'Segformer (HP)', 'Segformer (improved)'],\n",
        "    'Hamming Loss (Pixel Error)': [h_loss_unet_base, h_loss_unet_aug, h_loss_unet_hp, h_loss_unet_improved,\n",
        "                     h_loss_segformer_base, h_loss_segformer_aug, h_loss_segformer_hp, h_loss_segformer_improved],\n",
        "    'F1 Score (Weighted)': [f1_unet_base, f1_unet_aug, f1_unet_hp, f1_unet_improved,\n",
        "                 f1_segformer_base, f1_segformer_aug, f1_segformer_hp, f1_segformer_improved],\n",
        "    'mAP (mean IoU)': [mAP_unet_base, mAP_unet_aug, mAP_unet_hp, mAP_unet_improved,\n",
        "                       mAP_segformer_base, mAP_segformer_aug, mAP_segformer_hp, mAP_segformer_improved]\n",
        "}\n",
        "\n",
        "df_baseline_comparison = pd.DataFrame(results).round(4)\n",
        "print(\"\\nСравнение результатов предобученных моделей:\")\n",
        "print(tabulate(df_baseline_comparison, headers='keys', tablefmt='pipe'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "004b5d88-2c65-49d7-82d4-b0d459250ba1",
      "metadata": {
        "id": "004b5d88-2c65-49d7-82d4-b0d459250ba1"
      },
      "source": [
        "### Выводы по бейзлайн моделям\n",
        "\n",
        "На основе сравнения метрик в таблице можно сделать следующие выводы:\n",
        "\n",
        "*   Базовые модели (Unet base, Segformer base), обученные всего 5 эпох с LR 0.0001, показывают приемлемые, но не выдающиеся результаты. Unet base достиг mAP 0.9684, Segformer base - mAP 0.9711.\n",
        "*   Аугментация данных (Unet aug, Segformer aug) на этом этапе (с базовыми HPs и 5 эпохами) не привела к существенному улучшению, а в случае Unet даже немного ухудшила метрики (mAP 0.9658 для Unet aug, 0.9710 для Segformer aug). Возможно, для полного проявления эффекта аугментации требуется больше эпох обучения или другие гиперпараметры.\n",
        "*   Подбор гиперпараметров (Увеличение эпох до 10 и снижение LR до 0.00005, а также смена оптимизатора на AdamW для Segformer) оказал более существенное влияние (Unet HP, Segformer HP). Метрики значительно выросли по сравнению с базовым бейзлайном: Unet HP достиг mAP 0.9721, а Segformer HP - mAP 0.9727. Это подтверждает Гипотезу 2 и указывает на то, что предобученным моделям нужно больше времени и более тонкая настройка для адаптации к целевому датасету.\n",
        "*   Лучшие результаты для обеих архитектур достигнуты при комбинации аугментации и улучшенных гиперпараметров (Unet improved, Segformer improved). Unet improved достиг mAP 0.9752, а Segformer improved - mAP 0.9738. Это ожидаемо, поскольку эти техники дополняют друг друга: аугментация помогает с генерализацией, а тонкая настройка обучения позволяет модели лучше использовать увеличенный набор данных.\n",
        "*   Среди предобученных моделей, Unet (resnet34) в своем лучшем варианте (Unet improved с mAP 0.9752) показал немного лучшие результаты по сравнению с Segformer (mit_b0) в его лучшем варианте (Segformer improved с mAP 0.9738) на данном датасете с использованными параметрами. Это может говорить о том, что для этой задачи и с данным энкодером Unet оказался чуть эффективнее, хотя Segformer также показывает очень высокие метрики.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca3a7983-1f2f-404a-92ee-6edea6d8e368",
      "metadata": {
        "id": "ca3a7983-1f2f-404a-92ee-6edea6d8e368"
      },
      "source": [
        "## Имплементация упрощенных алгоритмов машинного обучения"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9871be8a-2099-4cfc-aeae-f572158c8747",
      "metadata": {
        "id": "9871be8a-2099-4cfc-aeae-f572158c8747"
      },
      "source": [
        "### Имплементации моделей"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a9e6bb5-8c91-4bbd-91ae-567bbb93f994",
      "metadata": {
        "id": "6a9e6bb5-8c91-4bbd-91ae-567bbb93f994"
      },
      "source": [
        "Упрощенная имплементация Unet (только 2 блока энкодера и 2 блока декодера, без skip-connections для максимального упрощения)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f61cdb04-390c-4a99-8fbd-a468581e095a",
      "metadata": {
        "id": "f61cdb04-390c-4a99-8fbd-a468581e095a",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class SimpleUNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(SimpleUNet, self).__init__()\n",
        "\n",
        "        self.conv1 = self.conv_block(in_channels, 32)\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = self.conv_block(32, 64)\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # Выход H/4, W/4\n",
        "\n",
        "        # Бутылочное горлышко (простое)\n",
        "        self.bottleneck = self.conv_block(64, 128)\n",
        "\n",
        "        # Декодер - апсемплинг с помощью ConvTranspose\n",
        "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
        "        self.dec_conv2 = self.conv_block(64, 64)\n",
        "\n",
        "        self.upconv1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
        "        self.dec_conv1 = self.conv_block(32, 32)\n",
        "\n",
        "        # Выходной слой\n",
        "        self.out_conv = nn.Conv2d(32, out_channels, kernel_size=1)\n",
        "\n",
        "    def conv_block(self, in_channels, out_channels):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Энкодер\n",
        "        x1 = self.conv1(x)\n",
        "        x = self.pool1(x1)\n",
        "\n",
        "        x2 = self.conv2(x)\n",
        "        x = self.pool2(x2) # Выход H/4 x W/4\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # Декодер (без skip-соединений)\n",
        "        x = self.upconv2(x) # Апсемплинг до H/2 x W/2\n",
        "        # Если используются skip-соединения, здесь нужно объединить x с x2\n",
        "        x = self.dec_conv2(x)\n",
        "\n",
        "        x = self.upconv1(x) # Апсемплинг до H x W\n",
        "        # Если используются skip-соединения, здесь нужно объединить x с x1\n",
        "        x = self.dec_conv1(x)\n",
        "\n",
        "        # Выход\n",
        "        out = self.out_conv(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b8d7500-d698-44a6-9fef-daf80e024724",
      "metadata": {
        "id": "3b8d7500-d698-44a6-9fef-daf80e024724"
      },
      "source": [
        "Упрощенная имплементация Vision Transformer для сегментации"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe3870a-9f69-416a-a9e3-4a9a8b15f5d3",
      "metadata": {
        "id": "6fe3870a-9f69-416a-a9e3-4a9a8b15f5d3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Базовый блок Transformer Encoder (упрощенный от стандартного ViT)\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        # Используем стандартный nn.MultiheadAttention\n",
        "        self.attn = nn.MultiheadAttention(dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Остаточные соединения\n",
        "        identity = x\n",
        "        x = self.norm1(x)\n",
        "        # MultiheadAttention возвращает (выход, веса) - берем только выход\n",
        "        x, _ = self.attn(x, x, x)\n",
        "        x = identity + x # Добавляем остаточное соединение\n",
        "\n",
        "        identity = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.mlp(x)\n",
        "        x = identity + x # Добавляем остаточное соединение\n",
        "        return x\n",
        "\n",
        "class SimpleViTSegmentation(nn.Module):\n",
        "    def __init__(self, image_size, patch_size, num_classes, dim, depth, num_heads, mlp_ratio, channels=3):\n",
        "        super().__init__()\n",
        "\n",
        "        assert image_size % patch_size == 0, 'Размеры изображения должны делиться на размер патча.'\n",
        "\n",
        "        self.image_size = image_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_classes = num_classes # Сохраняем num_classes\n",
        "        grid_size = image_size // patch_size\n",
        "        num_patches = grid_size * grid_size\n",
        "        patch_dim = channels * patch_size ** 2\n",
        "\n",
        "        self.patch_embedding = nn.Sequential(\n",
        "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size), # (B, num_patches, patch_dim)\n",
        "            nn.LayerNorm(patch_dim), # Добавляем LayerNorm перед линейной проекцией\n",
        "            nn.Linear(patch_dim, dim), # Проецируем признаки патча в размерность модели\n",
        "            # nn.LayerNorm(dim), # Опциональный LayerNorm после проекции\n",
        "        )\n",
        "\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches, dim))\n",
        "\n",
        "        # Энкодер Трансформера\n",
        "        self.transformer_encoder = nn.Sequential(*[\n",
        "            TransformerEncoderBlock(dim, num_heads, mlp_ratio)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        # Голова сегментации (Декодер)\n",
        "        self.segmentation_head = nn.Sequential(\n",
        "            nn.LayerNorm(dim), # Нормализация перед головой\n",
        "            nn.Linear(dim, self.num_classes) # Проецируем размерность токена в логиты классов для каждого патча\n",
        "        )\n",
        "\n",
        "    def forward(self, img):\n",
        "        b, c, h, w = img.shape\n",
        "\n",
        "        if h != self.image_size or w != self.image_size:\n",
        "             # Это должно обрабатываться преобразованиями датасета, но проверка полезна.\n",
        "             raise ValueError(f\"Размер входного изображения {(h, w)} не соответствует ожидаемому {self.image_size}\")\n",
        "\n",
        "        # 1. Встраивание патчей\n",
        "        x = self.patch_embedding(img) # (B, num_patches, dim)\n",
        "\n",
        "        # 2. Добавляем позиционное встраивание\n",
        "        x = x + self.pos_embedding # (B, num_patches, dim)\n",
        "\n",
        "        # 3. Энкодер Трансформера\n",
        "        x = self.transformer_encoder(x) # (B, num_patches, dim)\n",
        "\n",
        "        # 4. Голова сегментации\n",
        "        x = self.segmentation_head(x) # (B, num_patches, num_classes)\n",
        "\n",
        "        # 5. Меняем форму и апсемплим до исходного размера изображения\n",
        "        grid_size = self.image_size // self.patch_size\n",
        "\n",
        "        # Проверяем, соответствует ли количество патчей квадрату размера сетки\n",
        "        assert x.shape[1] == grid_size * grid_size, \"Несоответствие количества патчей\"\n",
        "\n",
        "        x = x.view(b, grid_size, grid_size, self.num_classes) # (B, grid_h, grid_w, num_classes)\n",
        "        x = x.permute(0, 3, 1, 2) # (B, num_classes, grid_h, grid_w)\n",
        "\n",
        "        # Апсемплинг до исходных размеров изображения (H, W)\n",
        "        x = nn.functional.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)\n",
        "\n",
        "        return x # (B, num_classes, H, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b8309d9-07c3-48dd-9816-d0ef0fc83fa6",
      "metadata": {
        "id": "5b8309d9-07c3-48dd-9816-d0ef0fc83fa6"
      },
      "source": [
        "### Обучение упрощенных моделей (Базовый вариант)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "simple_models_base_train_params",
      "metadata": {
        "id": "simple_models_base_train_params"
      },
      "source": [
        "Для базового сравнения имплементированных моделей с предобученными бейзлайнами, обучим их с нуля, используя **стандартный датасет без аугментации** и **базовые гиперпараметры**, аналогичные тем, что использовались для первых прогонов предобученных моделей (хотя для обучения с нуля часто требуются другие параметры). Это позволит увидеть \"сырую\" производительность простых архитектур без преимуществ предобучения или оптимизированных техник обучения.\n",
        "\n",
        "*   Количество эпох: 5\n",
        "*   Learning Rate: 0.001 (немного выше, чем базовый для предобученных, часто нужно больше для обучения с нуля)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "simple_unet_base_train_eval",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "simple_unet_base_train_eval",
        "outputId": "8a924d47-808a-4928-c5df-52b4b8e2a8ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Simple UNet (Impl base) ---\n",
            "EPOCH 1/5\n",
            "  Train Loss: 0.5843\n",
            "EPOCH 2/5\n",
            "  Train Loss: 0.5103\n",
            "EPOCH 3/5\n",
            "  Train Loss: 0.4862\n",
            "EPOCH 4/5\n",
            "  Train Loss: 0.4372\n",
            "EPOCH 5/5\n",
            "  Train Loss: 0.4164\n",
            "Training Simple UNet (Impl base) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.1588\n",
            "  F1 Score (Weighted): 0.8405\n",
            "  mAP (mean IoU): 0.7020\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Используем стандартный (не аугментированный) даталоадер\n",
        "train_loader_simple_base = train_loader_base\n",
        "\n",
        "SIMPLE_MODEL_BASE_LR = 0.001 # Начальный LR для обучения с нуля\n",
        "\n",
        "# Инициализация Simple UNet (начинаем с чистого листа)\n",
        "model_unet_simple_base = SimpleUNet(in_channels=3, out_channels=NUM_CLASSES).to(DEVICE)\n",
        "optimizer_unet_simple_base = optim.Adam(model_unet_simple_base.parameters(), lr=SIMPLE_MODEL_BASE_LR)\n",
        "\n",
        "h_loss_unet_simple_base, f1_unet_simple_base, mAP_unet_simple_base = train_and_evaluate_model(\n",
        "    model=model_unet_simple_base,\n",
        "    train_loader=train_loader_simple_base,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_unet_simple_base,\n",
        "    device=DEVICE,\n",
        "    epochs=BASE_EPOCHS, # Столько же эпох, как базовый бейзлайн\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Simple UNet (Impl base)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee3c40fd-1e24-49e9-aa2d-c896cc09a735",
      "metadata": {
        "id": "ee3c40fd-1e24-49e9-aa2d-c896cc09a735"
      },
      "source": [
        "Обучение и оценка Simple ViT (базовый)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "simple_vit_base_train_eval",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "simple_vit_base_train_eval",
        "outputId": "687a4342-be9c-46fa-d4df-06d784e43c4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Simple ViT (Impl base) ---\n",
            "EPOCH 1/5\n",
            "  Train Loss: 0.4044\n",
            "EPOCH 2/5\n",
            "  Train Loss: 0.3413\n",
            "EPOCH 3/5\n",
            "  Train Loss: 0.3320\n",
            "EPOCH 4/5\n",
            "  Train Loss: 0.3093\n",
            "EPOCH 5/5\n",
            "  Train Loss: 0.2906\n",
            "Training Simple ViT (Impl base) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.1249\n",
            "  F1 Score (Weighted): 0.8729\n",
            "  mAP (mean IoU): 0.7528\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Параметры Simple ViT (выбираем достаточно небольшие)\n",
        "vit_patch_size = 16\n",
        "vit_dim = 128 # Размерность токенов\n",
        "vit_depth = 3 # Количество слоев трансформера\n",
        "vit_heads = 4 # Количество голов внимания\n",
        "vit_mlp_ratio = 4.0 # Размерность скрытого слоя в MLP относительно dim\n",
        "\n",
        "model_vit_simple_base = SimpleViTSegmentation(\n",
        "    image_size=INPUT_DIM[0],\n",
        "    patch_size=vit_patch_size,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    dim=vit_dim,\n",
        "    depth=vit_depth,\n",
        "    num_heads=vit_heads,\n",
        "    mlp_ratio=vit_mlp_ratio,\n",
        "    channels=3\n",
        ").to(DEVICE)\n",
        "\n",
        "# Используем базовый LR и Adam\n",
        "optimizer_vit_simple_base = optim.Adam(model_vit_simple_base.parameters(), lr=SIMPLE_MODEL_BASE_LR)\n",
        "\n",
        "h_loss_vit_simple_base, f1_vit_simple_base, mAP_vit_simple_base = train_and_evaluate_model(\n",
        "    model=model_vit_simple_base,\n",
        "    train_loader=train_loader_simple_base,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_vit_simple_base,\n",
        "    device=DEVICE,\n",
        "    epochs=BASE_EPOCHS, # Столько же эпох, как базовый бейзлайн\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Simple ViT (Impl base)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c510a32c-69fc-44f3-8e08-845374149e0d",
      "metadata": {
        "id": "c510a32c-69fc-44f3-8e08-845374149e0d"
      },
      "source": [
        "### Сравнение результатов упрощенных моделей с базовыми (на стандартных параметрах)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "simple_vs_base_comparison",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "simple_vs_base_comparison",
        "outputId": "e79e3c17-f028-40e6-81cf-7304b7920c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Сравнение имплементированных моделей (базовое обучение) с предобученными бейзлайнами:\n",
            "|    | Model                   |   Hamming Loss (Pixel Error) |   F1 Score (Weighted) |   mAP (mean IoU) |\n",
            "|---:|:------------------------|-----------------------------:|----------------------:|-----------------:|\n",
            "|  0 | Unet (base)             |                       0.0145 |                0.9855 |           0.9684 |\n",
            "|  1 | Segformer (base)        |                       0.0133 |                0.9868 |           0.9711 |\n",
            "|  2 | Simple UNet (Impl base) |                       0.1588 |                0.8405 |           0.7020 |\n",
            "|  3 | Simple ViT (Impl base)  |                       0.1249 |                0.8729 |           0.7528 |\n"
          ]
        }
      ],
      "source": [
        "results_simple_base = {\n",
        "    'Model': ['Unet (base)', 'Segformer (base)', 'Simple UNet (Impl base)', 'Simple ViT (Impl base)'],\n",
        "    'Hamming Loss (Pixel Error)': [h_loss_unet_base, h_loss_segformer_base, h_loss_unet_simple_base, h_loss_vit_simple_base],\n",
        "    'F1 Score (Weighted)': [f1_unet_base, f1_segformer_base, f1_unet_simple_base, f1_vit_simple_base],\n",
        "    'mAP (mean IoU)': [mAP_unet_base, mAP_segformer_base, mAP_unet_simple_base, mAP_vit_simple_base]\n",
        "}\n",
        "\n",
        "df_simple_base_comparison = pd.DataFrame(results_simple_base).round(4)\n",
        "print(\"\\nСравнение имплементированных моделей (базовое обучение) с предобученными бейзлайнами:\")\n",
        "print(tabulate(df_simple_base_comparison, headers='keys', tablefmt='pipe'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aab08397-1a31-4b9c-a16c-deb696b0a1ed",
      "metadata": {
        "id": "aab08397-1a31-4b9c-a16c-deb696b0a1ed"
      },
      "source": [
        "### Выводы по упрощенным моделям (базовый вариант)\n",
        "\n",
        "На основе сравнения метрик можно сделать следующие выводы:\n",
        "\n",
        "*   Как и ожидалось, Simple модели, обученные с нуля всего за 5 эпох, показали значительно более низкое качество по всем метрикам по сравнению с предобученными моделями (Unet base с mAP 0.9684, Segformer base с mAP 0.9711).\n",
        "*   Simple UNet (Impl base) показал 0.7020 mAP, а Simple ViT (Impl base) - 0.7528 mAP. Эти значения очень низки, что говорит о том, что простым архитектурам требуется либо гораздо больше данных/эпох, либо предобучение, чтобы выучить полезные признаки для задачи сегментации.\n",
        "*   Интересно отметить, что Simple ViT (Impl base), несмотря на свою упрощенность, на базовом обучении показал немного лучшие результаты (mAP 0.7528) по сравнению с Simple UNet (Impl base) (mAP 0.7020).\n",
        "*   Основная причина низкой производительности имплементированных моделей на данном этапе - **отсутствие предобучения** на большом датасете, таком как ImageNet, которое дает предобученным моделям сильную базу для переноса знаний.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b724358d-ab7d-4255-a7cb-c252993a81e9",
      "metadata": {
        "id": "b724358d-ab7d-4255-a7cb-c252993a81e9"
      },
      "source": [
        "### Добавление техник из улучшенного бейзлайна к упрощенным моделям"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "improved_simple_models_description",
      "metadata": {
        "id": "improved_simple_models_description"
      },
      "source": [
        "Теперь применим к имплементированным моделям те техники, которые оказались наиболее успешными для предобученных бейзлайнов: **аугментацию данных** и **улучшенные гиперпараметры (пониженный LR, увеличенное число эпох, AdamW для ViT)**. Цель - проверить, насколько эти техники могут улучшить обучение моделей с нуля и сократить разрыв с предобученными моделями."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "improved_simple_models_params_data",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "improved_simple_models_params_data",
        "outputId": "25904a56-35a1-4888-8abb-6613ef572ac9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Настройки обучения имплементированных моделей (улучшенный): Аугментация, LR=5e-05, Epochs=10\n"
          ]
        }
      ],
      "source": [
        "# Используем параметры и даталоадеры из \"Улучшенного бейзлайна\"\n",
        "IMPROVED_SIMPLE_EPOCHS = FINAL_EPOCHS # 10 эпох\n",
        "IMPROVED_SIMPLE_LR = FINAL_LEARNING_RATE # 0.00005\n",
        "train_loader_improved_simple = train_loader_improved # Аугментированный даталоадер\n",
        "\n",
        "print(\"Настройки обучения имплементированных моделей (улучшенный): Аугментация, LR={IMPROVED_SIMPLE_LR}, Epochs={IMPROVED_SIMPLE_EPOCHS}\".format(IMPROVED_SIMPLE_LR=IMPROVED_SIMPLE_LR, IMPROVED_SIMPLE_EPOCHS=IMPROVED_SIMPLE_EPOCHS))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ac51520-5696-4a89-8682-6c7f2cd2e6f3",
      "metadata": {
        "id": "6ac51520-5696-4a89-8682-6c7fcd2e6f3"
      },
      "source": [
        "Обучение и оценка Simple UNet (улучшенный)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "simple_unet_improved_train_eval",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "simple_unet_improved_train_eval",
        "outputId": "05ce771a-f64e-47f8-c435-d8b04c62ee70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Simple UNet (Impl improved) ---\n",
            "EPOCH 1/10\n",
            "  Train Loss: 0.6314\n",
            "EPOCH 2/10\n",
            "  Train Loss: 0.5487\n",
            "EPOCH 3/10\n",
            "  Train Loss: 0.5119\n",
            "EPOCH 4/10\n",
            "  Train Loss: 0.4921\n",
            "EPOCH 5/10\n",
            "  Train Loss: 0.4648\n",
            "EPOCH 6/10\n",
            "  Train Loss: 0.4449\n",
            "EPOCH 7/10\n",
            "  Train Loss: 0.4247\n",
            "EPOCH 8/10\n",
            "  Train Loss: 0.4142\n",
            "EPOCH 9/10\n",
            "  Train Loss: 0.4006\n",
            "EPOCH 10/10\n",
            "  Train Loss: 0.3956\n",
            "Training Simple UNet (Impl improved) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.1686\n",
            "  F1 Score (Weighted): 0.8269\n",
            "  mAP (mean IoU): 0.6780\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Инициализация Simple UNet (начинаем с чистого листа)\n",
        "model_unet_simple_improved = SimpleUNet(in_channels=3, out_channels=NUM_CLASSES).to(DEVICE)\n",
        "optimizer_unet_simple_improved = optim.Adam(model_unet_simple_improved.parameters(), lr=IMPROVED_SIMPLE_LR)\n",
        "\n",
        "h_loss_unet_simple_improved, f1_unet_simple_improved, mAP_unet_simple_improved = train_and_evaluate_model(\n",
        "    model=model_unet_simple_improved,\n",
        "    train_loader=train_loader_improved_simple,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_unet_simple_improved,\n",
        "    device=DEVICE,\n",
        "    epochs=IMPROVED_SIMPLE_EPOCHS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Simple UNet (Impl improved)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59680a64-4c01-46fc-8cee-2f4aa3babd6d",
      "metadata": {
        "id": "59680a64-4c01-46fc-8cee-2f4aa3babd6d"
      },
      "source": [
        "Обучение и оценка Simple ViT (улучшенный)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "simple_vit_improved_train_eval",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "simple_vit_improved_train_eval",
        "outputId": "e7b9227b-e4c9-44fc-f5df-406fc7a53a6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Training Simple ViT (Impl improved) ---\n",
            "EPOCH 1/10\n",
            "  Train Loss: 0.5027\n",
            "EPOCH 2/10\n",
            "  Train Loss: 0.3998\n",
            "EPOCH 3/10\n",
            "  Train Loss: 0.3775\n",
            "EPOCH 4/10\n",
            "  Train Loss: 0.3628\n",
            "EPOCH 5/10\n",
            "  Train Loss: 0.3487\n",
            "EPOCH 6/10\n",
            "  Train Loss: 0.3409\n",
            "EPOCH 7/10\n",
            "  Train Loss: 0.3325\n",
            "EPOCH 8/10\n",
            "  Train Loss: 0.3268\n",
            "EPOCH 9/10\n",
            "  Train Loss: 0.3213\n",
            "EPOCH 10/10\n",
            "  Train Loss: 0.3149\n",
            "Training Simple ViT (Impl improved) finished.\n",
            "Evaluating on test set...\n",
            "  Hamming Loss (Pixel Error): 0.1339\n",
            "  F1 Score (Weighted): 0.8639\n",
            "  mAP (mean IoU): 0.7378\n",
            "--------------------\n"
          ]
        }
      ],
      "source": [
        "# Инициализация Simple ViT (начинаем с чистого листа)\n",
        "model_vit_simple_improved = SimpleViTSegmentation(\n",
        "    image_size=INPUT_DIM[0],\n",
        "    patch_size=vit_patch_size,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    dim=vit_dim,\n",
        "    depth=vit_depth,\n",
        "    num_heads=vit_heads,\n",
        "    mlp_ratio=vit_mlp_ratio,\n",
        "    channels=3\n",
        ").to(DEVICE)\n",
        "\n",
        "# Используем AdamW для трансформера с пониженным LR и weight decay\n",
        "optimizer_vit_simple_improved = optim.AdamW(model_vit_simple_improved.parameters(), lr=IMPROVED_SIMPLE_LR, weight_decay=0.01)\n",
        "\n",
        "h_loss_vit_simple_improved, f1_vit_simple_improved, mAP_vit_simple_improved = train_and_evaluate_model(\n",
        "    model=model_vit_simple_improved,\n",
        "    train_loader=train_loader_improved_simple,\n",
        "    val_loader=test_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer_vit_simple_improved,\n",
        "    device=DEVICE,\n",
        "    epochs=IMPROVED_SIMPLE_EPOCHS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    ignore_index=IGNORE_INDEX,\n",
        "    model_name=\"Simple ViT (Impl improved)\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "722b6ad8-d3db-476b-9ce8-73f89692d1b9",
      "metadata": {
        "id": "722b6ad8-d3db-476b-9ce8-73f89692d1b9"
      },
      "source": [
        "### Сравнение результатов улучшенных моделей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "improved_comparison_table",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "improved_comparison_table",
        "outputId": "266ab382-009a-4b1b-faed-804af5b17898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Сравнение улучшенных моделей (предобученные vs имплементированные):\n",
            "|    | Model                       |   Hamming Loss (Pixel Error) |   F1 Score (Weighted) |   mAP (mean IoU) |\n",
            "|---:|:----------------------------|-----------------------------:|----------------------:|-----------------:|\n",
            "|  0 | Unet (improved)             |                       0.0113 |                0.9887 |           0.9752 |\n",
            "|  1 | Simple UNet (Impl improved) |                       0.1686 |                0.8269 |           0.6780 |\n",
            "|  2 | Segformer (improved)        |                       0.0119 |                0.9881 |           0.9738 |\n",
            "|  3 | Simple ViT (Impl improved)  |                       0.1339 |                0.8639 |           0.7378 |\n"
          ]
        }
      ],
      "source": [
        "results_improved = {\n",
        "    'Model': ['Unet (improved)', 'Simple UNet (Impl improved)', 'Segformer (improved)', 'Simple ViT (Impl improved)'],\n",
        "    'Hamming Loss (Pixel Error)': [h_loss_unet_improved, h_loss_unet_simple_improved, h_loss_segformer_improved, h_loss_vit_simple_improved],\n",
        "    'F1 Score (Weighted)': [f1_unet_improved, f1_unet_simple_improved, f1_segformer_improved, f1_vit_simple_improved],\n",
        "    'mAP (mean IoU)': [mAP_unet_improved, mAP_unet_simple_improved, mAP_segformer_improved, mAP_vit_simple_improved]\n",
        "}\n",
        "\n",
        "df_improved_comparison = pd.DataFrame(results_improved).round(4)\n",
        "print(\"\\nСравнение улучшенных моделей (предобученные vs имплементированные):\")\n",
        "print(tabulate(df_improved_comparison, headers='keys', tablefmt='pipe'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9da4a37f-e4c2-4f12-b3c5-18b25e636e7c",
      "metadata": {
        "id": "9da4a37f-e4c2-4f12-b3c5-18b25e636e7c"
      },
      "source": [
        "### Выводы\n",
        "\n",
        "На основе сравнения метрик всех четырех финальных вариантов моделей можно сделать следующие выводы:\n",
        "\n",
        "*   После применения техник из улучшенного бейзлайна (аугментация, пониженный LR, увеличенное число эпох, AdamW для трансформеров), качество предобученных моделей значительно улучшилось, достигнув mAP 0.9752 для Unet (improved) и 0.9738 для Segformer (improved).\n",
        "*   Однако, имплементированные Simple модели (Simple UNet Impl improved: 0.6780 mAP, Simple ViT Impl improved: 0.7378 mAP), при обучении с нуля с теми же \"улучшенными\" гиперпараметрами, показали **снижение** качества по сравнению с их базовым обучением (Simple UNet Impl base: 0.7020 mAP, Simple ViT Impl base: 0.7528 mAP). Этот неожиданный результат говорит о том, что гиперпараметры и техники, оптимальные для дообучения предобученных моделей (где LR должен быть низким для тонкой настройки), не подходят для обучения простых моделей с нуля. Модели, обучаемые с нуля, вероятно, требуют более высокого Learning Rate и, возможно, других аугментаций или регуляризации на начальных этапах обучения.\n",
        "*   Даже с \"улучшенными\" техниками обучения, имплементированные Simple модели по-прежнему существенно уступают предобученным моделям из библиотеки. Это убедительно демонстрирует **критическую важность предобучения** для задач сегментации на сравнительно небольших датасетах, таких как Oxford-IIIT Pet. Предобученные модели обладают сильными базовыми признаками, которые легко адаптируются под новую задачу, в то время как модели, обучаемые с нуля, требуют огромного объема данных и/или значительно большего времени/ресурсов для достижения схожего уровня понимания изображений.\n",
        "*   Среди предобученных моделей, Unet (improved) показал немного лучший результат (0.9752 mAP) по сравнению с Segformer (improved) (0.9738 mAP) на данном датасете с использованными параметрами. Это может говорить о том, что классическая сверточная архитектура с эффективным энкодером (ResNet34) хорошо справляется с этой задачей, возможно, лучше адаптируясь к локальным особенностям изображения, чем трансформерная модель с более глобальным вниманием (Segformer mit_b0).\n",
        "*   Среди имплементированных моделей, Simple ViT (Impl improved) показал немного лучшие метрики (0.7378 mAP) по сравнению с Simple UNet (Impl improved) (0.6780 mAP), хотя обе модели показали снижение качества по сравнению с базовым обучением. Это может быть связано с тем, что даже упрощенная трансформерная архитектура имеет потенциал для выучивания более мощных представлений, но ее обучение с нуля более чувствительно к выбору гиперпараметров и техник, чем упрощенная сверточная архитектура.\n",
        "*   В целом, лабораторная работа показала, что для достижения высокого качества семантической сегментации на Oxford-IIIT Pet наиболее эффективным подходом является дообучение **предобученных моделей** из библиотеки, сочетая **аугментацию данных** и **тонкую настройку гиперпараметров (особенно Learning Rate и количество эпох)**. Имплементированные модели, хотя и могут улучшаться от правильных техник обучения с нуля, не могут преодолеть разрыв в производительности без предобучения или значительно более сложной архитектуры и большего количества данных/ресурсов для обучения с нуля. Применение техник fine-tuning к обучению с нуля может даже навредить, как показали результаты с Simple моделями.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
